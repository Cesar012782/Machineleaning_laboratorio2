{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **Lab II - Clustering**\n",
    "## Spectral Clustering ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Research about the Spectral Clustering method, and answer the following questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El **Spectral Clustering** es un algoritmo de agrupación que aprovecha la conectividad entre puntos de datos representándolos como nodos de un grafo, en el que las aristas denotan similitud. El algoritmo consta de dos pasos principales: \n",
    "1. la construcción de un grafo de similitud \n",
    "2. la proyección de los datos en un espacio de dimensiones inferiores mediante la matriz laplaciana del grafo.\n",
    " \n",
    "El grafo de similitud se construye a partir de un enfoque de vecindad épsilon, de k vecinos más próximos o de conexión completa. El objetivo de la proyección es acercar a los miembros del mismo cluster en el espacio reducido. La matriz laplaciana del grafo, un componente crucial en este proceso, se calcula definiendo el grado de cada nodo a partir de la matriz de adyacencia. El algoritmo resulta útil cuando los enfoques tradicionales de agrupación pueden tener dificultades debido a la dispersión de los puntos de datos en dimensiones superiores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. In which cases might it be more useful to apply?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Conglomerados no esféricos:** Cuando los datos forman naturalmente conglomerados con formas irregulares (por ejemplo, alargados, en forma de media luna), la agrupación espectral sobresale en comparación con algoritmos como K-means que asumen conglomerados esféricos.\n",
    "* **Datos de alta dimensión:** Al proyectar los datos en un espacio de menor dimensión, la agrupación espectral aborda el problema de la \"maldición de la dimensionalidad\" que obstaculiza otros algoritmos en dimensiones elevadas.\n",
    "* **Descubrimiento de relaciones ocultas:** Debido a su enfoque basado en gráficos, la agrupación espectral puede descubrir conexiones sutiles entre puntos de datos más allá de la simple proximidad, lo que conduce a agrupaciones potencialmente significativas.\n",
    "* **Data with inherent structure:** When data naturally exhibits a graph-like structure (e.g., social networks, text documents), spectral clustering leverages this structure for effective clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. What are the mathematical fundamentals of it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El **Spectral clustering** se basa en varios conceptos matemáticos clave:\n",
    "\n",
    "* **Graph theory:** representa los puntos de datos como nodos de un grafo, en el que las aristas conectan puntos similares. Esto permite aprovechar las conexiones entre puntos para la agrupación.\n",
    "* **Linear algebra:** Los valores propios y los vectores propios de la matriz laplaciana, derivados de la matriz de similitud, desempeñan un papel crucial en la reducción de la dimensionalidad y la identificación de conglomerados.\n",
    "* **Spectral graph theory:** Este campo de las matemáticas proporciona la base para analizar los valores propios y los vectores propios de los grafos, lo que resulta esencial para la agrupación espectral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C. What is the algorithm to compute it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo para calcularlo consta de los siguientes pasos:\n",
    "\n",
    "1. **Construct a similarity matrix:** Esta matriz expresa la similitud entre cada par de puntos de datos.\n",
    "2. **Build a graph:** Representar los puntos de datos como nodos y conectar los puntos similares con aristas basadas en la matriz de similitud.\n",
    "3. **Compute the Laplacian matrix:** Esta matriz captura la estructura y la conectividad del grafo.\n",
    "4. **Find eigenvalues and eigenvectors:** Calcule los vectores propios correspondientes a los k valores propios más pequeños distintos de cero de la matriz laplaciana.\n",
    "5. **Project data onto lower dimensions:** Utilice los vectores propios seleccionados para proyectar los puntos de datos en un espacio de k dimensiones.\n",
    "6. **Apply clustering:** Utilice un algoritmo de agrupación estándar (por ejemplo, K-means) en el espacio de dimensiones inferiores para agrupar los puntos de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D. Does it hold any relation to some of the concepts previously mentioned in class? Which, and how?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sí, la agrupación espectral está relacionada con varios conceptos tratados en clase:\n",
    "\n",
    "* **Métricas de distancia:** Las matrices de similitud en la agrupación espectral amplían el concepto de métrica de distancia al capturar relaciones más complejas que la simple distancia euclidiana.\n",
    "* **Reducción de la dimensionalidad:** Al proyectar los datos en un espacio de menor dimensión, la agrupación espectral comparte aspectos con técnicas como el análisis de componentes principales (ACP), pero aprovecha la estructura de grafos para la proyección.\n",
    "* **Aprendizaje no supervisado:** Al igual que K-means y otros algoritmos de clustering, el clustering espectral pertenece al aprendizaje no supervisado, agrupando puntos de datos sin etiquetas previas.\n",
    "* **Algoritmos de grafos:** La construcción y el análisis del grafo en la agrupación espectral se basan en conceptos de algoritmos de grafos como la detección de comunidades.\n",
    "\n",
    "Teniendo en cuenta que no estuve n la primera clase asumo por los temas tratdo que si tuvieron ue haberlo aborado "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Research about the DBSCAN method, and answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. In which cases might it be more useful to apply?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EL algritmo **DBSCAN** es útil en varias situaciones:\n",
    "\n",
    "* **Clusters of arbitrary shapes:** A diferencia de K-Means y los clústeres jerárquicos, que tienen dificultades con los clústeres no esféricos, DBSCAN destaca en la identificación de clústeres con densidades variables y formas irregulares.\n",
    "* **Handling noise:** DBSCAN identifica y separa eficazmente los puntos de datos atípicos (ruido) de los conglomerados reales gracias a su enfoque basado en la densidad.\n",
    "* **Uncertain number of clusters:** A diferencia de K-Means, que requiere clusters predefinidos, DBSCAN determina automáticamente el número de clusters en función de la densidad de los datos, lo que lo hace adecuado para el análisis exploratorio de datos.\n",
    "* **High-dimensional data:** Aunque es sensible al ajuste de los parámetros, DBSCAN puede funcionar bien en dimensiones elevadas sin necesidad de reducir la dimensionalidad como en el clustering espectral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B.What are the mathematical fundamentals of it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **DBSCAN** funciona según el principio de agrupamiento basado en la densidad, en el que los clusters se consideran regiones densas separadas por áreas de menor densidad.\n",
    "* Los parámetros clave son épsilon (ε), el radio alrededor de cada punto de datos, y minPuntos, el número mínimo de puntos de datos necesarios dentro del radio épsilon para que un punto se clasifique como Corepoint.\n",
    "* Tres conceptos definen las relaciones entre los puntos de datos en DBSCAN:\n",
    "    * **Directly Density-Reachable:** Un punto X es directamente accesible desde el punto Y si X pertenece a la vecindad de Y (distancia <= epsilon) e Y es un punto central.\n",
    "    * **Directly Density-Reachable:** X es alcanzable por la densidad desde Y si existe una cadena de puntos directamente alcanzables por la densidad desde X hasta Y.\n",
    "    * **Density-Connected:** X e Y están conectados por densidad si existe un punto O tal que tanto X como Y son alcanzables por densidad desde O."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C.Is there any relation between DBSCAN and Spectral Clustering? If so, what is it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tanto DBSCAN como Spectral Clustering son algoritmos de agrupación, pero funcionan según principios diferentes.\n",
    "* DBSCAN es un algoritmo de clustering basado en la densidad que identifica los clusters a partir de la densidad local de los puntos de datos, y no requiere que se especifique de antemano el número de clusters.\n",
    "* El clustering espectral, por su parte, funciona a partir de la descomposición espectral de la matriz de similitud e implica la creación de clusters en un espacio de dimensiones inferiores.\n",
    "* Aunque ambos algoritmos se utilizan para la agrupación, tienen puntos fuertes diferentes y son adecuados para distintos tipos de conjuntos de datos. El DBSCAN es robusto frente a los valores atípicos y eficaz para conglomerados de forma irregular, mientras que el Clustering Espectral puede manejar formas no convexas y es útil cuando la conectividad es importante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. What is the elbow method in clustering? And which flaws does it pose to assess quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método del codo es una técnica visual utilizada en la agrupación, en particular con el algoritmo K-Means, para determinar el número óptimo de grupos (k) para un conjunto de datos determinado. Funciona de la siguiente manera:\n",
    "\n",
    "* **Perform K-Means clustering with various k values:** Ejecute el algoritmo K-Means de forma iterativa, aumentando el número de conglomerados (k) desde 1 hasta un valor máximo elegido.\n",
    "* **Calculate the Within-Cluster Sum of Squares (WCSS) for each k:** La WCSS mide la distancia total entre los puntos de datos dentro de cada conglomerado y su respectivo centroide. Una WCSS más baja indica conglomerados más cerrados.\n",
    "* **Plot the WCSS against the corresponding k values:** Esto crea un gráfico lineal.\n",
    "* **Identify the \"elbow\" point:** Busque el punto en el que la línea empieza a aplanarse significativamente, formando un \"codo\". Este punto representa aproximadamente el valor óptimo de k, en el que añadir más conglomerados no mejora significativamente la calidad de la agrupación.\n",
    "\n",
    "Aunque el método del codo es sencillo e intuitivo, tiene varias limitaciones:\n",
    "\n",
    "* **Subjectivity:** La identificación del punto \"codo\" puede ser subjetiva y depender de la interpretación individual. Diferentes analistas pueden elegir distintos valores de k basándose en el mismo gráfico.\n",
    "* **Not always clear elbow:** A veces, el gráfico carece de un codo claro, lo que dificulta la determinación del valor óptimo de k.\n",
    "* **Sensitive to data scaling:** El WCSS se ve afectado por la escala de las características de los datos. Diferentes métodos de escalado pueden conducir a diferentes ubicaciones del codo.\n",
    "Asume conglomerados esféricos: El método funciona mejor para datos con conglomerados esféricos. Es posible que no identifique con precisión el k óptimo para conglomerados con formas irregulares.\n",
    "* **Not statistically sound:** El método del codo no proporciona ninguna medida estadística de la calidad de los conglomerados. Se basa únicamente en la inspección visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Remember the unsupervised Python package you created in the previous unit? It’s time for an upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Implement the k-means module using Python and Numpy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. Implement the k-medoids module using Python and Numpy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C. Remember to keep consistency with Scikit-Learn API as high as possible**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Let’s use the newly created modules in unsupervised to cluster some toy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Use the following code snippet to create scattered data X**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs \n",
    "X, y = make_blobs(\n",
    "    n_samples=500, \n",
    "    n_features=2,\n",
    "    centers=4,\n",
    "    cluster_std=1, \n",
    "    center_box=(-10.0, 10.0), \n",
    "    shuffle=True, \n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. Plot the resulting dataset. How many clusters are there? How far are they from one another?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C. For both k-means and k-medoids (your implementations), calculate the silhouette plots and oefficients for each run, iterating K from 1 to 5 clusters.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D. What number of K got the best silhouette score? What can you say about the figures? Is this the expected result?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Use the following code snippet to create different types of scattered data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import cluster, datasets, mixture\n",
    "# ============\n",
    "# Generate datasets. We choose the size big enough to see the scalability\n",
    "# of the algorithms, but not too big to avoid too long running times\n",
    "# ============\n",
    "n_samples = 500\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=0.5, noise=0.05) noisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "# Anisotropically distributed data\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state) \n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "# blobs with varied variances \n",
    "varied = datasets.make_blobs(\n",
    "n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Plot the different datasets in separate figures. What can you say about them?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. Apply k-means, k-medoids, DBSCAN and Spectral Clustering from Scikit-Learn over each dataset and compare the results of each algorithm with respect to each dataset.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
